{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5XWG7WngQo_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEp9A9326-4q",
        "outputId": "40411773-d974-4f77-dccd-debdb5e53507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/fer2013\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resnet34"
      ],
      "metadata": {
        "id": "QJLpAAjKxq1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import zipfile\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -- Dataset Setup --\n",
        "BASE_DIR = '/kaggle/input/fer2013'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "entries = os.listdir(BASE_DIR)\n",
        "print(\"Entries in BASE_DIR:\", entries)\n",
        "\n",
        "# Folder-based structure\n",
        "if 'train' in entries and 'test' in entries:\n",
        "    print(\"Using folder-based dataset...\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize((48,48)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    train_folder = os.path.join(BASE_DIR, 'train')\n",
        "    test_folder  = os.path.join(BASE_DIR, 'test')\n",
        "    full_train = ImageFolder(train_folder, transform=transform)\n",
        "    train_size = int(0.9 * len(full_train))\n",
        "    val_size   = len(full_train) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        full_train, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    test_dataset = ImageFolder(test_folder, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=128, shuffle=False, num_workers=4)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "    class_names = full_train.classes\n",
        "    num_classes = len(class_names)\n",
        "    use_test = True\n",
        "\n",
        "# CSV/ZIP-based fallback\n",
        "else:\n",
        "    print(\"Using CSV/ZIP-based dataset...\")\n",
        "    files = entries\n",
        "    csv_file = next((f for f in files if f.endswith('.csv')), None)\n",
        "    if csv_file is None:\n",
        "        zip_file = next((f for f in files if f.endswith('.zip')), None)\n",
        "        if zip_file is None:\n",
        "            raise FileNotFoundError(\"Không tìm thấy CSV hoặc ZIP FER2013 trong BASE_DIR.\")\n",
        "        with zipfile.ZipFile(os.path.join(BASE_DIR, zip_file), 'r') as z:\n",
        "            z.extractall(BASE_DIR)\n",
        "        files = os.listdir(BASE_DIR)\n",
        "        csv_file = next(f for f in files if f.endswith('.csv'))\n",
        "    csv_path = os.path.join(BASE_DIR, csv_file)\n",
        "    print(\"Loading CSV:\", csv_path)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    pixels = df['pixels'].tolist()\n",
        "    labels = df['emotion'].tolist()\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        pixels, labels, test_size=0.1, stratify=labels, random_state=42\n",
        "    )\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((48,48)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    class FER2013Dataset(Dataset):\n",
        "        def __init__(self, px_list, lbl_list, transform=None):\n",
        "            self.px = px_list\n",
        "            self.lbl = lbl_list\n",
        "            self.transform = transform\n",
        "        def __len__(self): return len(self.lbl)\n",
        "        def __getitem__(self, idx):\n",
        "            arr = np.fromstring(self.px[idx], sep=' ', dtype=np.uint8).reshape(48,48)\n",
        "            img = Image.fromarray(arr).convert('L')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, self.lbl[idx]\n",
        "\n",
        "    train_dataset = FER2013Dataset(X_train, y_train, transform)\n",
        "    val_dataset   = FER2013Dataset(X_val,   y_val,   transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "    class_names = [str(i) for i in sorted(set(labels))]\n",
        "    num_classes = len(class_names)\n",
        "    use_test = False\n",
        "\n",
        "# -- Simple CNN Model Definition --\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 6 * 6, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN(num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# -- Training Loop --\n",
        "num_epochs = 20\n",
        "history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    loop = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "    for imgs, labels in loop:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        loop.set_postfix(batch_loss=loss.item())\n",
        "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    val_loss_total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            preds = model(imgs)\n",
        "            batch_loss = criterion(preds, labels)\n",
        "            val_loss_total += batch_loss.item() * imgs.size(0)\n",
        "            correct += (preds.argmax(dim=1) == labels).sum().item()\n",
        "    avg_val_loss = val_loss_total / len(val_loader.dataset)\n",
        "    val_acc = correct / len(val_loader.dataset)\n",
        "    print(f\"Epoch {epoch} -- Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\\n\")\n",
        "\n",
        "    # Store history\n",
        "    history['epoch'].append(epoch)\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "# -- Display training history as table --\n",
        "hist_df = pd.DataFrame(history)\n",
        "print(\"\\nTraining History:\")\n",
        "print(hist_df.to_string(index=False))\n",
        "\n",
        "# -- Final Reports & Save --\n",
        "all_preds, all_labels = [], []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        preds = model(imgs).argmax(dim=1).cpu().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.tolist())\n",
        "print(\"\\nClassification Report on Validation:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "if use_test:\n",
        "    print(\"\\nEvaluating on Test Set...\")\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            preds = model(imgs).argmax(dim=1).cpu().tolist()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.tolist())\n",
        "    print(f\"Test Accuracy: {np.mean(np.array(all_preds) == np.array(all_labels)):.4f}\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "out_path = 'simple_cnn_fer2013.pth'\n",
        "torch.save(model.state_dict(), out_path)\n",
        "print(f\"Model saved to {out_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tFEopkH_A9w",
        "outputId": "2866229a-3983-4398-87b1-bd495e1110f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entries in BASE_DIR: ['test', 'train']\n",
            "Using folder-based dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/20: 100%|██████████| 202/202 [02:15<00:00,  1.49it/s, batch_loss=1.49]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -- Train Loss: 1.7054, Val Loss: 1.5612, Val Acc: 0.3835\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 2/20: 100%|██████████| 202/202 [02:16<00:00,  1.47it/s, batch_loss=1.25]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 -- Train Loss: 1.4951, Val Loss: 1.4086, Val Acc: 0.4626\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 3/20: 100%|██████████| 202/202 [02:12<00:00,  1.53it/s, batch_loss=1.39]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 -- Train Loss: 1.3729, Val Loss: 1.2922, Val Acc: 0.4936\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 4/20: 100%|██████████| 202/202 [02:19<00:00,  1.45it/s, batch_loss=1.26]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 -- Train Loss: 1.2972, Val Loss: 1.2611, Val Acc: 0.5162\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 5/20: 100%|██████████| 202/202 [02:17<00:00,  1.47it/s, batch_loss=1.35]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 -- Train Loss: 1.2380, Val Loss: 1.2081, Val Acc: 0.5402\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 6/20: 100%|██████████| 202/202 [02:21<00:00,  1.43it/s, batch_loss=1.12]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 -- Train Loss: 1.1870, Val Loss: 1.1787, Val Acc: 0.5479\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 7/20: 100%|██████████| 202/202 [02:16<00:00,  1.47it/s, batch_loss=1.01]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 -- Train Loss: 1.1436, Val Loss: 1.1694, Val Acc: 0.5507\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 8/20: 100%|██████████| 202/202 [02:17<00:00,  1.47it/s, batch_loss=1.03]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 -- Train Loss: 1.1139, Val Loss: 1.1597, Val Acc: 0.5674\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 9/20: 100%|██████████| 202/202 [02:17<00:00,  1.47it/s, batch_loss=1.08]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 -- Train Loss: 1.0797, Val Loss: 1.1363, Val Acc: 0.5758\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 10/20: 100%|██████████| 202/202 [02:17<00:00,  1.47it/s, batch_loss=0.918]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 -- Train Loss: 1.0518, Val Loss: 1.1227, Val Acc: 0.5796\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 11/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 11/20: 100%|██████████| 202/202 [02:18<00:00,  1.46it/s, batch_loss=1.05]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 -- Train Loss: 1.0200, Val Loss: 1.1297, Val Acc: 0.5712\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 12/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 12/20: 100%|██████████| 202/202 [02:16<00:00,  1.48it/s, batch_loss=0.904]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 -- Train Loss: 0.9915, Val Loss: 1.1398, Val Acc: 0.5709\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 13/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 13/20: 100%|██████████| 202/202 [02:15<00:00,  1.49it/s, batch_loss=1.07]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 -- Train Loss: 0.9588, Val Loss: 1.1210, Val Acc: 0.5820\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 14/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 14/20: 100%|██████████| 202/202 [02:17<00:00,  1.47it/s, batch_loss=1.03]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 -- Train Loss: 0.9318, Val Loss: 1.1145, Val Acc: 0.5859\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 15/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 15/20: 100%|██████████| 202/202 [02:18<00:00,  1.46it/s, batch_loss=0.938]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 -- Train Loss: 0.8963, Val Loss: 1.1295, Val Acc: 0.5866\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 16/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 16/20: 100%|██████████| 202/202 [02:17<00:00,  1.47it/s, batch_loss=0.956]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 -- Train Loss: 0.8808, Val Loss: 1.1590, Val Acc: 0.5719\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 17/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 17/20: 100%|██████████| 202/202 [02:16<00:00,  1.48it/s, batch_loss=0.861]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 -- Train Loss: 0.8552, Val Loss: 1.1589, Val Acc: 0.5862\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 18/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 18/20: 100%|██████████| 202/202 [02:15<00:00,  1.49it/s, batch_loss=0.85]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 -- Train Loss: 0.8322, Val Loss: 1.1614, Val Acc: 0.5775\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 19/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 19/20: 100%|██████████| 202/202 [02:15<00:00,  1.49it/s, batch_loss=0.638]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 -- Train Loss: 0.8059, Val Loss: 1.1528, Val Acc: 0.5873\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 20/20:   0%|          | 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 20/20: 100%|██████████| 202/202 [02:15<00:00,  1.49it/s, batch_loss=0.924]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 -- Train Loss: 0.7828, Val Loss: 1.1801, Val Acc: 0.5949\n",
            "\n",
            "\n",
            "Training History:\n",
            " epoch  train_loss  val_loss  val_acc\n",
            "     1    1.705364  1.561231 0.383490\n",
            "     2    1.495112  1.408559 0.462557\n",
            "     3    1.372897  1.292234 0.493556\n",
            "     4    1.297161  1.261112 0.516196\n",
            "     5    1.238031  1.208079 0.540230\n",
            "     6    1.187049  1.178747 0.547893\n",
            "     7    1.143619  1.169405 0.550679\n",
            "     8    1.113902  1.159710 0.567398\n",
            "     9    1.079663  1.136285 0.575758\n",
            "    10    1.051811  1.122659 0.579589\n",
            "    11    1.019979  1.129691 0.571230\n",
            "    12    0.991499  1.139807 0.570881\n",
            "    13    0.958768  1.120986 0.582027\n",
            "    14    0.931808  1.114486 0.585859\n",
            "    15    0.896291  1.129459 0.586555\n",
            "    16    0.880783  1.159016 0.571926\n",
            "    17    0.855153  1.158891 0.586207\n",
            "    18    0.832244  1.161444 0.577499\n",
            "    19    0.805876  1.152816 0.587252\n",
            "    20    0.782822  1.180095 0.594915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report on Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.52      0.49      0.51       376\n",
            "     disgust       0.74      0.32      0.44        44\n",
            "        fear       0.47      0.42      0.44       424\n",
            "       happy       0.81      0.80      0.80       732\n",
            "     neutral       0.49      0.62      0.55       510\n",
            "         sad       0.47      0.46      0.46       474\n",
            "    surprise       0.74      0.73      0.73       311\n",
            "\n",
            "    accuracy                           0.60      2871\n",
            "   macro avg       0.61      0.55      0.56      2871\n",
            "weighted avg       0.60      0.60      0.60      2871\n",
            "\n",
            "\n",
            "Evaluating on Test Set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5846\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.53      0.47      0.50       958\n",
            "     disgust       0.82      0.29      0.43       111\n",
            "        fear       0.41      0.37      0.39      1024\n",
            "       happy       0.79      0.79      0.79      1774\n",
            "     neutral       0.51      0.62      0.56      1233\n",
            "         sad       0.45      0.44      0.45      1247\n",
            "    surprise       0.70      0.75      0.72       831\n",
            "\n",
            "    accuracy                           0.58      7178\n",
            "   macro avg       0.60      0.53      0.55      7178\n",
            "weighted avg       0.58      0.58      0.58      7178\n",
            "\n",
            "Model saved to simple_cnn_fer2013.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resnet18"
      ],
      "metadata": {
        "id": "58FoPcp_ydKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== 1. Import Libraries =====\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import zipfile\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n"
      ],
      "metadata": {
        "id": "PhZjwIEVybFO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 2. Download & Unpack Dataset =====\n",
        "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "BASE_DIR = path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVoVKHYlyhox",
        "outputId": "7ed0424a-cb12-4fc9-e070-443b20e3c614"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/fer2013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 3. Setup Device =====\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "H1-lo7J_yhi6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 4. Define Transforms =====\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ],
      "metadata": {
        "id": "nngH9h19yhe_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 5. Load Dataset =====\n",
        "entries = os.listdir(BASE_DIR)\n",
        "print(\"Entries in BASE_DIR:\", entries)\n",
        "\n",
        "if 'train' in entries and 'test' in entries:\n",
        "    # Folder-based\n",
        "    train_folder = os.path.join(BASE_DIR, 'train')\n",
        "    test_folder  = os.path.join(BASE_DIR, 'test')\n",
        "    full_train = ImageFolder(train_folder, transform=transform)\n",
        "    train_size = int(0.9 * len(full_train))\n",
        "    val_size   = len(full_train) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        full_train, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    test_dataset = ImageFolder(test_folder, transform=transform)\n",
        "    use_test = True\n",
        "else:\n",
        "    # CSV/ZIP fallback\n",
        "    csv_file = next((f for f in entries if f.endswith('.csv')), None)\n",
        "    if not csv_file:\n",
        "        zip_file = next((f for f in entries if f.endswith('.zip')), None)\n",
        "        with zipfile.ZipFile(os.path.join(BASE_DIR, zip_file), 'r') as z:\n",
        "            z.extractall(BASE_DIR)\n",
        "        entries = os.listdir(BASE_DIR)\n",
        "        csv_file = next(f for f in entries if f.endswith('.csv'))\n",
        "    df = pd.read_csv(os.path.join(BASE_DIR, csv_file))\n",
        "    pixels, labels = df['pixels'].tolist(), df['emotion'].tolist()\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        pixels, labels, test_size=0.1, stratify=labels, random_state=42\n",
        "    )\n",
        "    class FER2013Dataset(Dataset):\n",
        "        def __init__(self, px_list, lbl_list, transform=None):\n",
        "            self.px = px_list\n",
        "            self.lbl = lbl_list\n",
        "            self.transform = transform\n",
        "        def __len__(self): return len(self.lbl)\n",
        "        def __getitem__(self, idx):\n",
        "            arr = np.fromstring(self.px[idx], sep=' ', dtype=np.uint8).reshape(48,48)\n",
        "            img = Image.fromarray(arr).convert('L')\n",
        "            return self.transform(img), self.lbl[idx]\n",
        "    train_dataset = FER2013Dataset(X_train, y_train, transform)\n",
        "    val_dataset   = FER2013Dataset(X_val,   y_val,   transform)\n",
        "    use_test = False\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 128\n",
        "if use_test:\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "else:\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "class_names = full_train.classes if 'full_train' in locals() else [str(i) for i in sorted(set(labels))]\n",
        "num_classes = len(class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHmmsUevzI1U",
        "outputId": "dc3c3718-9596-451a-f1a8-e4bf6cba98d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entries in BASE_DIR: ['test', 'train']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 6. Define ResNet-18 Model =====\n",
        "model = models.resnet18(pretrained=False)\n",
        "# Adjust input channel & output classes\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "model.fc    = nn.Linear(model.fc.in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33k9-x-zLRY",
        "outputId": "5ded46f8-d5f5-4cae-d955-9403503b5897"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 7. Training Loop =====\n",
        "num_epochs = 20\n",
        "history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    loop = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "    for imgs, labels in loop:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(imgs), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        loop.set_postfix(batch_loss=loss.item())\n",
        "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            val_loss += criterion(outputs, labels).item() * imgs.size(0)\n",
        "            correct  += (outputs.argmax(1) == labels).sum().item()\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_acc = correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch} -- Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    history['epoch'].append(epoch)\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    history['val_acc'].append(val_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBz9vRg1zOCG",
        "outputId": "71037f88-c59c-4c21-e0e8-906acd7855ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 202/202 [08:21<00:00,  2.48s/it, batch_loss=1.47]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 -- Train Loss: 1.5199, Val Loss: 1.3930, Val Acc: 0.4674\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 202/202 [08:13<00:00,  2.44s/it, batch_loss=1.26]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 -- Train Loss: 1.3089, Val Loss: 1.3134, Val Acc: 0.4963\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 202/202 [08:24<00:00,  2.50s/it, batch_loss=1.1]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 -- Train Loss: 1.1930, Val Loss: 1.2689, Val Acc: 0.5120\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 202/202 [08:35<00:00,  2.55s/it, batch_loss=1.07]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 -- Train Loss: 1.1214, Val Loss: 1.1748, Val Acc: 0.5542\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 202/202 [08:31<00:00,  2.53s/it, batch_loss=1.03]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 -- Train Loss: 1.0493, Val Loss: 1.1879, Val Acc: 0.5514\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 202/202 [08:43<00:00,  2.59s/it, batch_loss=1.06]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 -- Train Loss: 0.9839, Val Loss: 1.2782, Val Acc: 0.5246\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 202/202 [08:38<00:00,  2.56s/it, batch_loss=0.988]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 -- Train Loss: 0.9174, Val Loss: 1.1786, Val Acc: 0.5691\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 202/202 [08:32<00:00,  2.54s/it, batch_loss=0.733]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 -- Train Loss: 0.8553, Val Loss: 1.2302, Val Acc: 0.5538\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 202/202 [08:36<00:00,  2.56s/it, batch_loss=0.873]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 -- Train Loss: 0.7913, Val Loss: 1.1649, Val Acc: 0.5883\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 202/202 [08:37<00:00,  2.56s/it, batch_loss=0.773]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 -- Train Loss: 0.7131, Val Loss: 1.2493, Val Acc: 0.5803\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 202/202 [08:38<00:00,  2.57s/it, batch_loss=0.761]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 -- Train Loss: 0.6462, Val Loss: 1.3085, Val Acc: 0.5611\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 202/202 [08:36<00:00,  2.56s/it, batch_loss=0.552]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 -- Train Loss: 0.5715, Val Loss: 1.3200, Val Acc: 0.5747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 202/202 [08:30<00:00,  2.53s/it, batch_loss=0.561]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 -- Train Loss: 0.5171, Val Loss: 1.3782, Val Acc: 0.5622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 202/202 [08:27<00:00,  2.51s/it, batch_loss=0.369]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 -- Train Loss: 0.4504, Val Loss: 1.5284, Val Acc: 0.5632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 202/202 [08:27<00:00,  2.51s/it, batch_loss=0.485]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 -- Train Loss: 0.3976, Val Loss: 1.5069, Val Acc: 0.5646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 202/202 [08:30<00:00,  2.53s/it, batch_loss=0.602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 -- Train Loss: 0.3481, Val Loss: 1.5620, Val Acc: 0.5765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 202/202 [08:35<00:00,  2.55s/it, batch_loss=0.284]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 -- Train Loss: 0.3020, Val Loss: 1.6286, Val Acc: 0.5737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 202/202 [08:33<00:00,  2.54s/it, batch_loss=0.203]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 -- Train Loss: 0.2604, Val Loss: 1.7091, Val Acc: 0.5831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 202/202 [08:33<00:00,  2.54s/it, batch_loss=0.215]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 -- Train Loss: 0.2345, Val Loss: 1.8088, Val Acc: 0.5646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 202/202 [08:28<00:00,  2.52s/it, batch_loss=0.239]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 -- Train Loss: 0.2116, Val Loss: 1.9446, Val Acc: 0.5569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 8. Evaluation & Save =====\n",
        "all_preds, all_labels = [], []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        preds = model(imgs.to(device)).argmax(1).cpu().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "if 'test_loader' in locals():\n",
        "    test_preds, test_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            preds = model(imgs.to(device)).argmax(1).cpu().tolist()\n",
        "            test_preds.extend(preds); test_labels.extend(labels)\n",
        "    print(f\"Test Accuracy: {(np.array(test_preds)==np.array(test_labels)).mean():.4f}\")\n",
        "\n",
        "out_path = 'resnet18_fer2013.pth'\n",
        "torch.save(model.state_dict(), out_path)\n",
        "print(f\"Model saved to {out_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc063kQAzTg3",
        "outputId": "5724952c-43b5-4305-baa6-f9453390441b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.38      0.56      0.45       376\n",
            "     disgust       0.55      0.36      0.44        44\n",
            "        fear       0.47      0.48      0.48       424\n",
            "       happy       0.83      0.70      0.76       732\n",
            "     neutral       0.47      0.34      0.39       510\n",
            "         sad       0.41      0.49      0.44       474\n",
            "    surprise       0.73      0.73      0.73       311\n",
            "\n",
            "    accuracy                           0.55      2871\n",
            "   macro avg       0.55      0.52      0.53      2871\n",
            "weighted avg       0.57      0.55      0.55      2871\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5549\n",
            "Model saved to resnet18_fer2013.pth\n"
          ]
        }
      ]
    }
  ]
}